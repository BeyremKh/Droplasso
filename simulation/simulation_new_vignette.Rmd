---
title: "Droplasso vs Glmnet and LiblineaR on simulated data"
author: "Jean-Philippe Vert"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: bibli.bib
header-includes:
    - \newcommand{\RR}{\mathbb{R}}
    - \usepackage{amsmath,amssymb,amsfont,bbm}
vignette: >
  %\VignetteIndexEntry{Droplasso vs Glmnet on simulated data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, dev = "png", fig.keep = "high", fig.path = "figure/", cache.path = "cache/", fig.width=4, fig.height=3.4)
set.seed(4392)
library(parallel)
ncores <- detectCores() -1 

```

# Introduction  
`droplasso` is a package that fits a generalized linear model via maximum likelihood regularized by droplasso [@Khalfaoui2018DropLasso], a procedure that combines dropout [@Srivastava2014Dropout] and lasso [@Tibshirani1996Regression] regularizations.

Given a training set of samples $x_1,\ldots,x_n\in\mathbb{R}^d$ with labels $y_1,\ldots,y_n\in\mathbb{R}$, the droplasso regularization estimates a linear model $f(x)=w^\top x$ for $x\in\mathbb{R}^d$ by solving:
\begin{equation}
  \min_{w \in \mathbb{R}^d}   \left\{ \frac{1}{n} \sum_{i=1}^{n} \underset{\delta_i \sim B(p)^d}{ \mathbb {E}}  L\left(w,\delta_i  \odot \frac{x_{i,}}{p}  , y_{i} \right)   +  \lambda  \left \| w\right \|_{1} \right\}  \,,
\end{equation}
where $L(w,x,y)$ is a negative log likelihood of a linear model $w$ on a observation $x$ with label $y$, $B(p)$ is the Bernoulli distribution with parameter $p$, and $\odot$ is the entry-wise multiplication of vectors. When $p=1$, each $\delta_i$ is almost surely a vector of $1$'s and droplasso boils down to classical lasso; and when $\lambda=0$, droplasso boils down to dropout.

# Data simulation
Here we illustrate the use of droplasso on simulated data, and compare it to standard dropout and elastic net regularisation. We design a toy simulation to illustrate in particular how corruption by dropout noise impacts the performances of the different methods. The simulation goes as follow : 

- We set the dimension to $d=20$.
- Each sample is a random vector $z\in\mathbb{N}^d$ with entries following a Poisson distribution with parameter $\pi=1$. The data variables are independent. 
- The "true" model is a logistic model with sparse weight vector $w\in\mathbb{R}^d$ satisfying $w_1=+5$, $w_2=-5$,  and $w_i=0$ for $i=3,\ldots,d.$
- Using $w$ as the true underlying model and $z$ as the true observations, we simulate a label $y \sim B( 1/(1+\exp(-w^\top z )) )$  
- We introduce corruption in the samples by dropout events by multiplying entry-wise $z$ with an i.i.d Bernoulli variables $\delta$ with probability $q$, this probability can be chosen also and will vary through next experiments. 

Let us simulate $n=100$ samples to form the training set, and $10,000$ samples to test the model:
```{r simulation_setting}
library(mvtnorm)
generate_data <- function(n=100, d=20, d1=1, pi=1, w=10 , q=1) {
  # The samples 
  mu <- c(rep(0,d))
  Sigma <-  diag(d)
  rawvars <- rmvnorm(n, mean=mu, sigma=Sigma)
  pvars <- pnorm(rawvars)  
  z  <- qpois(pvars, pi)

  w <-c(rep(w,d1),rep(-w,d1),rep(0,d-2*d1))
  # The labels y
  y <- rbinom(n, 1, 1/(1+exp(-z %*%  w)) )
  # The corrupted samples x
  x <-sapply(1:d, function(i) z[,i] * rbinom(n,1,q))

  return(list(z=z, x=x, y=y, w=w))
}
data_train <- generate_data()
data_valid <- generate_data(n=10000)
data_test <- generate_data(n=10000)

```

# ElasticNet and variable selection

Let's first see the performance of Elasticnet on simulated data without noise: 
```{r glmnet perf}
library(glmnet)
library(ROCR)
nlambda <- 10

alphalist <- seq(0,1,0.1)
auclist <- c()
firstalpha <- TRUE
for (alpha in alphalist) {
  # Train glmnet model
  m_glm <- glmnet(data_train$x, data_train$y, family="binomial", intercept=F, alpha=alpha, standardize = F, nlambda=nlambda)
  # Predict on the validation set
  ypred <- data_valid$x %*% m_glm$beta
  pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
  auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
  if (length(auc) < nlambda) { # glmnet did not converge for the last lambda values
    auc <- c(auc, rep(0.5, nlambda-length(auc)))
  }
  if (firstalpha) {
    auc_el <- auc
    firstalpha <- FALSE
  } else {
    auc_el <- cbind(auc_el, auc)
  }
  
  # Assess AUC of the best lambda on the test set
  bestlambdaindex <- which.max(auc)
  ypred <- data_test$x %*% m_glm$beta[,bestlambdaindex]
  auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
  auclist <- c(auclist,auc)
}
plot(alphalist, auclist[1:11], type='b', xlab='alpha', ylab='Test AUC', main='Glmnet with best lambda')

print(paste("at alpha=1 the accuracy is ",auclist[length(auclist)], "It is for the best lambda that is ", m_glm$lambda[bestlambdaindex]))

matplot(auc_el, type='l', lty=1, xlab='lambda index', ylab='Validation AUC', main="Glmnet, different alpha's", legend=alphalist)
grid()

```


One can notice that the ElasticNet is best performing when it is selecting variables, this can be explained by the presence of the completely noisy variables in this simulation setting.

# Droplasso and variable selection


Let's now see how the DropLasso behaves for non-noisy and noisy simulations settings respectively. 

```{r droplasso perf_no_noise}
n_passes = 10000
probalist <- 0.6^seq(0,10)

auclist <- c()
for (proba in probalist) {
  # Train on the train set
  m <- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=proba, nlambda=nlambda, n_passes = n_passes,decay=0.1)
  # Pick lambda with the best AUC on the validation set
  ypred <- predict(m, data_valid$x)
  pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
  auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
  bestlambda <- m$lambda[which.max(auc)]
  # Assess AUC of the best lambda on the test set
  ypred <- predict(m, data_test$x, s=bestlambda)
  auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
  auclist <- c(auclist,auc)
}
plot(log(probalist), auclist, type='b', xlab='ln(p)', ylab='Test AUC', main='Droplasso: AUC vs p for best lambda')

print(paste("at p=1 the accuracy is ",auclist[1], ". It is for the best lambda that is ", bestlambda))

```

We can see that for a data set without noise (what we simulated with q=1), DropLasso performs best without dropout. Let's test dropLasso now with noise '

```{r droplasso perf_noise}
n_passes = 10000
probalist <- 0.6^seq(0,10)

#sampling a noisy dataset
data_train <- generate_data(q=0.4)
data_valid <- generate_data(n=10000,q=0.4)
data_test <- generate_data(n=10000,q=0.4)

auclist <- c()
for (proba in probalist) {
  # Train on the train set
  m <- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=proba, nlambda=nlambda, n_passes = n_passes,decay=0.1)
  # Pick lambda with the best AUC on the validation set
  ypred <- predict(m, data_valid$x)
  pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
  auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
  bestlambda <- m$lambda[which.max(auc)]
  # Assess AUC of the best lambda on the test set
  ypred <- predict(m, data_test$x, s=bestlambda)
  auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
  auclist <- c(auclist,auc)
}
plot(log(probalist), auclist, type='b', xlab='ln(p)', ylab='Test AUC', main='Droplasso: AUC vs p for best lambda')

print(paste("at p=1 the accuracy is ",auclist[1], ". It is for the best lambda that is ", bestlambda))

```

It is interesting to remark that droplasso makes use of dorpout only in the case where the simulation itself contins dropout events, that is a zero that hides a nonzero value. 

##  Comparing Droplasso and Glmnet: 10 runs without noise
We now procceed to a comparaison between DropLasso, dropout and ElasticNet on our Simulation.
In order to make a fair comparaison, we repeat the following on multiple runs (nruns) : 
- Generate a training, a validation and a test dataset from the simulation setting.
- Perform a grid search and select the best parameters for each method on the validation set.
- Evaluate and average the accuracy of each method using the best selected parameters.
See code [here](https://github.com/BeyremKh/Droplasso-experiments/blob/master/simulation/compare_simul.R)

```{r compare_function}
#dependecies 
library(parallel)
ncores <- detectCores() -1 
source("compare_simul.R")

```


```{r compare_processing}

```
## Experiment runs 


```{r compare_no_noise}
#10 runs accuracy comparaison with no noise
proc_compare(nruns=10, q=1)

```

```{r compare_small_noise}
#10 runs accuracy comparaison with noise ration = 0.4
proc_compare(nruns=10, q=0.4)
```

```{r compare_big_noise}
#10 runs accuracy comparaison with noise ration = 0.1
proc_compare(nruns=10, q=0.1)
```

We can see that ElasticNet outperforms droplasso when the simulations do not contain noise or do contain moderate noise. However droplasso significantly outperforms ElasticNet when the simulations contain more and more noise.
# References
