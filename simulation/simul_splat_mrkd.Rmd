---
title: "splatterexperiments"
author: "Beyrem"
date: "12/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(SummarizedExperiment)
library(splatter)

generate_data <- function(n=20100, d=500, droprob=-100) {
  # The samples 
sim.groups <- splatSimulate(batchCells=n,nGenes=d,dropout.type="experiment",dropout.mid=0,dropout.shape=droprob,group.prob = c(0.5, 0.5), method = "groups",verbose = FALSE,seed=sample(1:100000, 1))
  # The labels y
y=as.numeric(colData(sim.groups)['Group'][,1])-1
  return(list(x=t(counts(sim.groups)), y=y))
}
data <- generate_data()
data_train <- list(x=data$x[1:100,], y=data$y[1:100])
data_valid <- list(x=data$x[101:10100,], y=data$y[101:10100])
data_test <- list(x=data$x[10101:20100,], y=data$y[10101:20100])

n=10000
d=10000

droprob=-1000000
print(paste("average dropout fraction for example for q= ",droprob, " is :  ", sum(assays(splatSimulate(batchCells=n,nGenes=d,dropout.type="experiment",dropout.mid=0,dropout.shape=droprob,group.prob = c(0.5, 0.5), method = "groups",verbose = FALSE,seed=sample(1:100000, 1)))$Dropout/(n*d) )))

droprob=-1
print(paste("average dropout fraction for example for q= ",droprob, " is :  ", sum(assays(splatSimulate(batchCells=n,nGenes=d,dropout.type="experiment",dropout.mid=0,dropout.shape=droprob,group.prob = c(0.5, 0.5), method = "groups",verbose = FALSE,seed=sample(1:100000, 1)))$Dropout/(n*d) )))

droprob=-0.5
print(paste("average dropout fraction for example for q= ",droprob, " is :  ", sum(assays(splatSimulate(batchCells=n,nGenes=d,dropout.type="experiment",dropout.mid=0,dropout.shape=droprob,group.prob = c(0.5, 0.5), method = "groups",verbose = FALSE,seed=sample(1:100000, 1)))$Dropout/(n*d) )))

droprob=0.5
print(paste("average dropout fraction for example for q= ",droprob, " is :  ", sum(assays(splatSimulate(batchCells=n,nGenes=d,dropout.type="experiment",dropout.mid=0,dropout.shape=droprob,group.prob = c(0.5, 0.5), method = "groups",verbose = FALSE,seed=sample(1:100000, 1)))$Dropout/(n*d) )))


```


## glmnet

Try first glmnet
```{r glmnet, echo=FALSE}
library(glmnet)
library(ROCR)
nlambda <- 10
alphalist <- seq(0,1,0.1)
auclist <- c()
firstalpha <- TRUE
for (alpha in alphalist) {
  # Train glmnet model
  m_glm <- glmnet(data_train$x, data_train$y, family="binomial", intercept=F, alpha=alpha, standardize = F, nlambda=nlambda)
  # Predict on the validation set
  ypred <- data_valid$x %*% m_glm$beta
  pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
  auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
  if (length(auc) < nlambda) { # glmnet did not converge for the last lambda values
    auc <- c(auc, rep(0.5, nlambda-length(auc)))
  }
  if (firstalpha) {
    auc_el <- auc
    firstalpha <- FALSE
  } else {
    auc_el <- cbind(auc_el, auc)
  }
  
  # Assess AUC of the best lambda on the test set
  bestlambdaindex <- which.max(auc)
  ypred <- data_test$x %*% m_glm$beta[,bestlambdaindex]
  auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
  auclist <- c(auclist,auc)
}
plot(alphalist, auclist[1:11], type='b', xlab='alpha', ylab='Test AUC', main='Glmnet with best lambda')

print(paste("at alpha=1 the accuracy is ",auclist[length(auclist)], "It is for the best lambda that is ", m_glm$lambda[bestlambdaindex]))
```
Try first glmnet
```{r glmnet_alphas, echo=FALSE}
library(glmnet)
library(ROCR)
nlambda <- 10

alphalist <- seq(0,1,0.1)
auclist <- c()
firstalpha <- TRUE
for (alpha in alphalist) {
  # Train glmnet model
  m_glm <- glmnet(data_train$x, data_train$y, family="binomial", intercept=F, alpha=alpha, standardize = F, nlambda=nlambda)
  # Predict on the validation set
  ypred <- data_valid$x %*% m_glm$beta
  pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
  auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
  if (length(auc) < nlambda) { # glmnet did not converge for the last lambda values
    auc <- c(auc, rep(0.5, nlambda-length(auc)))
  }
  if (firstalpha) {
    auc_el <- auc
    firstalpha <- FALSE
  } else {
    auc_el <- cbind(auc_el, auc)
  }
  
  # Assess AUC of the best lambda on the test set
  bestlambdaindex <- which.max(auc)
  ypred <- data_test$x %*% m_glm$beta[,bestlambdaindex]
  auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
  auclist <- c(auclist,auc)
}
plot(alphalist, auclist[1:11], type='b', xlab='alpha', ylab='Test AUC', main='Glmnet with best lambda')

print(paste("at alpha=1 the accuracy is ",auclist[length(auclist)], "It is for the best lambda that is ", m_glm$lambda[bestlambdaindex]))
matplot(auc_el, type='l', lty=1, xlab='lambda index', ylab='Validation AUC', main="Glmnet, different alpha's", legend=alphalist)
grid()
```

```{r glmnet_alphas2, echo=FALSE}
library(droplasso)

n_passes = 1000
probalist <- 0.6^seq(0,10)

auclist <- c()
for (proba in probalist) {
  # Train on the train set
  m <- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=proba, nlambda=nlambda, n_passes = n_passes,decay=0.1)
  # Pick lambda with the best AUC on the validation set
  ypred <- predict(m, data_valid$x)
  pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
  auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
  bestlambda <- m$lambda[which.max(auc)]
  # Assess AUC of the best lambda on the test set
  ypred <- predict(m, data_test$x, s=bestlambda)
  auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
  auclist <- c(auclist,auc)
}
plot(log(probalist), auclist, type='b', xlab='ln(p)', ylab='Test AUC', main='Droplasso: AUC vs p for best lambda')

print(paste("at p=1 the accuracy is ",auclist[1], ". It is for the best lambda that is ", bestlambda))
```

```{r compare_code, echo=FALSE}
library(parallel)
ncores=detectCores()-1
#Comparaison function
compare <- function(nruns=10 , nlambda=10,   alphalist= seq(0,1,0.1), n_passes=1000, q=0){  
  
  #preparing accuracies tables
  auc_tot_el=matrix(0,nrow=11,ncol=10)
  auc_tot_dl=matrix(0,nrow=11,ncol=10)
  auc_tot_drop=matrix(0,nrow=11,ncol=1)
  
  
  tot_auc <- mclapply(1:nruns,function(iexp){
    data <- generate_data(droprob = q)
    data_train <- list(x=data$x[1:100,], y=data$y[1:100])
    data_valid <- list(x=data$x[101:10100,], y=data$y[101:10100])
    data_test <- list(x=data$x[10101:20100,], y=data$y[10101:20100])

    d=nrow(data_train)
    # The values of alpha to interpolate between lasso (alpha=1) and ridge (alpha=0)
    firstalpha <- TRUE
    for (ind in seq(length(alphalist))) {
      # Train glmnet model
      m_glm <- glmnet(data_train$x, data_train$y, family="binomial", intercept=F, alpha=alphalist[ind], standardize = F, nlambda=nlambda)
      # Predict on the validation set
      ypred <- data_valid$x %*% m_glm$beta
      pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
      auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
      auc_tot_el[ind,] <-  c(auc,rep(0.5, nlambda-length(auc))) 
    }
    # Assess AUC of the best lambda on the test set for ElasticNet
    lambdas=m_glm$lambda
    bestindex <- which(auc_tot_el == max(auc_tot_el), arr.ind = TRUE)[1,]
    m_glm <- glmnet(data_train$x, data_train$y, family="binomial", intercept=F, alpha=alphalist[bestindex[1]], standardize = F, nlambda=10)
    cat(m_glm$beta[,max(bestindex[2],length(m_glm$lambda))])
    ypred <- data_test$x %*% m_glm$beta[,max(bestindex[2],length(m_glm$lambda))]
    auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
    auclist_el <- auc
    # Assess AUC of the best lambda on the test set for LASSO 
    bestindex <- which(auc_tot_el[length(alphalist),] == max(auc_tot_el[length(alphalist),]))
    
    #train droplasso
    # The values of p we want to test
    probalist <- 0.6^seq(0,10)
    # The number of lambda values we want to test to form the regularization path
    # The number of epoch of the optimization procedure
    
    for (ind in seq(length(probalist))) {
      # Train on the train set
      m <- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=probalist[ind], nlambda=nlambda, n_passes = n_passes,decay=0.1)
      # Pick lambda with the best AUC on the validation set
      ypred <- predict(m, data_valid$x)
      pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
      auc <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
      auc_tot_dl[ind,] <-  auc 
      m_drop <- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=probalist[ind], lambda=0 , n_passes = n_passes,decay=0.1)
      ypred <- predict(m_drop, data_valid$x)
      pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
      auc_tot_drop[ind] <- unlist(lapply(pred, function(p) {performance(p, "auc")@y.values[[1]]}))
    }
    # Assess AUC of the best lambda on the test set
    bestindex <- which(auc_tot_dl == max(auc_tot_dl), arr.ind = TRUE)[1,]
    m <- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=probalist[bestindex[1]], nlambda=nlambda, n_passes = n_passes,decay=0.1)
    ypred <- predict(m, data_test$x,s=m$lambda[max(bestindex[2])])
    auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
    auclist_dl <- auc
    bestdrop <- which.max(auc_tot_drop)
    m<- droplasso(data_train$x, data_train$y, family="binomial", keep_prob=probalist[bestdrop], lambda=0, n_passes = n_passes,decay=0.1)
    ypred <- predict(m, data_test$x)
    pred <- sapply(seq(ncol(ypred)), function(j) {prediction(ypred[,j], data_valid$y)})
    auc <- performance(prediction(ypred, data_test$y), "auc")@y.values[[1]]
    auclist_drop <- auc
    return(c(auclist_el,auclist_dl,auclist_drop))
  },mc.cores=ncores)
  return(sapply(tot_auc, unlist))
}



#processing results:  
proc_compare <- function(nruns=10,q=-100, nlambda=10,   alphalist= seq(0,1,0.1), n_passes=1000){
  tot_auc = compare(nruns=nruns,q=q,  nlambda=nlambda,   alphalist= alphalist, n_passes=n_passes)  
  auclist_dl=tot_auc[1,]
  auclist_el=tot_auc[2,]
  auclist_drop=tot_auc[3,]
  
  
  #p_auc= t.test(auclist_el, auclist_dl,var.equal=(var.test(auclist_el, auclist_dl)$p.value>0.05),paired=T)$p.value
  
  mean_acc_el=mean(auclist_el)
  mean_acc_dl=mean(auclist_dl)
  mean_acc_drop=mean(auclist_drop)
  
  
  
  sd_acc_el=sd(auclist_el)
  sd_acc_dl=sd(auclist_dl)
  sd_acc_drop=sd(auclist_drop)
  
  print(paste("the best average AUC for Elasticnet is ",mean_acc_el," with a standard deviation of ",sd_acc_el))
  print(paste("the best average AUC for DropLasso is ",mean_acc_dl," with a standard deviation of ",sd_acc_dl))
  print(paste("the best average AUC for dropout is ", mean_acc_drop," with a standard deviation of ",sd_acc_drop))
  #print(paste("the p-value between DropLasso and ElasticNet is ", p_auc))
  
  
  boxplot(cbind(auclist_dl,auclist_drop,auclist_el),ylab="AUC",xlab="lambda index", col=rainbow(3),main=paste("Test AUC, noise rate:",q))
}

```


```{r compare_nonoise, echo=FALSE}
proc_compare(nruns=10, n_passes=1000)
```


```{r compare_noise, echo=FALSE}
proc_compare(nruns=10, q=0.1, n_passes=1000)
```



```{r compare_high_noise, echo=FALSE}
proc_compare(nruns=10, q=0.2, n_passes=1000)
```


```{r compare_high_high_noise, echo=FALSE}
proc_compare(nruns=10, q=0.5, n_passes=1000)
```
